{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"metals.dat\", names=True, dtype=None)\n",
    "\n",
    "# print out all columns we just got for free\n",
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's see what this data looks like\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'],data['FE_H'],s=20,c='black')\n",
    "\n",
    "# Cool! The data actually looks pretty linear so we can assume the function we are fitting *is* linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's perform a linear least squares regression on this data. \n",
    "# the function we'll be using comes from the stats module of scipy.\n",
    "# the cool thing is that we don't need to define a function, just plug in the data and go!\n",
    "\n",
    "fit = stats.linregress(data['R_GC'],data['FE_H'])\n",
    "print (fit.slope)\n",
    "print (fit.intercept)\n",
    "print (fit.rvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(5,20,1000)\n",
    "\n",
    "# let's make a function the normal way\n",
    "def line(x,m,b):\n",
    "    return m*x+b\n",
    "\n",
    "\n",
    "# let's make the same function the more confusing and abstract way\n",
    "line_l = lambda x,m,b: m*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'],data['FE_H'],s=20,c='black')\n",
    "ax.plot(x,func(x,fit.slope,fit.intercept))\n",
    "\n",
    "# Exercise, set limits of the plot which make sense and play with the alpha values.\n",
    "# label the axises (x = radius and y = metals)\n",
    "# makes sure you axis labels and tick marks are the right sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We seem to have an outlier so let's look at the residuals to confirm\n",
    "residual = data['FE_H']-func(data['R_GC'],fit.slope,fit.intercept)\n",
    "x = np.linspace(5,20,1000)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'],residual,s=15,c='black')\n",
    "ax.plot(x,x*0.0,color='purple')\n",
    "ax.set_xlim(6,16)\n",
    "\n",
    "outlier = np.absolute(residual)>0.2\n",
    "ax.scatter(data['R_GC'][outlier],residual[outlier],s=60,edgecolor='cyan',facecolor='none')\n",
    "\n",
    "# Yep! that's an outlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Mask the data to exclude the outlier and rerun the linear regression. \n",
    "# plot both fits on the same plot to compare them.\n",
    "\n",
    "fit_new = stats.linregress(data['R_GC'][~outlier],data['FE_H'][~outlier])\n",
    "print (fit.slope)\n",
    "print (fit.intercept)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'],data['FE_H'],s=20,c='black')\n",
    "ax.plot(x,line(x,fit.slope,fit.intercept))\n",
    "ax.plot(x,line(x,fit_new.slope,fit_new.intercept))\n",
    "\n",
    "# check the corelation coefficient for the data set without the outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the cutoff for the outliers and compare the function and the correlation coefficeients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the data does have error bars, which are a good idea to plot.\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'],data['FE_H'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['R_GC'],data['FE_H'],yerr=data['FE_H_ERR'],fmt='k.',ecolor='blue',zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(x,line(x,fit.slope,fit.intercept))\n",
    "ax.plot(x,line(x,fit_new.slope,fit_new.intercept))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise is to do the same thing for the oxygen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge exercise to do the same thing (including removal of the outliers) using curve_fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
