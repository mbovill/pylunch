{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"metals.dat\", names=True, dtype=None)\n",
    "\n",
    "# print out all columns we just got for free\n",
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make a function the normal way\n",
    "def linear(x, m, b):\n",
    "    return m*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back what we found last time\n",
    "# remember our values for the raw data were:\n",
    "# slope:    -0.058187\n",
    "# intercept: 0.565351\n",
    "popt, pcov = opt.curve_fit(linear, data['R_GC'], data['FE_H'])\n",
    "print(\"raw: \", *popt)\n",
    "\n",
    "# we can use curve_fit to de-weight outliers automatically\n",
    "# remember our values after we removed the outliers were:\n",
    "# slope:    -0.043537\n",
    "# intercept: 0.395084\n",
    "\n",
    "# note, result is dependent on choice of loss function\n",
    "# need to see method to trf in order to use least_squares instead of leastsq\n",
    "popt_clean, pcov = opt.curve_fit(linear, data['R_GC'], data['FE_H'], method='trf', \n",
    "                                 loss='arctan', f_scale=0.20)\n",
    "print(\"outliers accounted for: \", *popt_clean)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'],data['FE_H'],s=20,c='black',zorder=2)\n",
    "# cleaner error bars than last time, play around with the parameters\n",
    "ax.errorbar(data['R_GC'],data['FE_H'],yerr=data['FE_H_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['R_GC'],linear(data['R_GC'], *popt), label='raw')\n",
    "ax.plot(data['R_GC'],linear(data['R_GC'], *popt_clean), label='better')\n",
    "\n",
    "ax.set_xlabel('R$_{GC}$', fontsize=18)\n",
    "ax.set_ylabel('[Fe/H]', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets account for the errors\n",
    "# by pasing sigma, scipy knows to weight data according to their errors \n",
    "# naturally, sigma is expected to be the same size and correspond to y\n",
    "# notice: this only accounts for x error\n",
    "popt_err, pcov = opt.curve_fit(linear, data['R_GC'], data['FE_H'], sigma=data['FE_H_ERR'])\n",
    "print(*popt_err)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'],data['FE_H'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['R_GC'],data['FE_H'],yerr=data['FE_H_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['R_GC'],linear(data['R_GC'], *popt), label='raw')\n",
    "ax.plot(data['R_GC'],linear(data['R_GC'], *popt_clean), label='better')\n",
    "ax.plot(data['R_GC'],linear(data['R_GC'], *popt_err), label='errors')\n",
    "\n",
    "ax.set_xlabel('R$_{GC}$', fontsize=18)\n",
    "ax.set_ylabel('[Fe/H]', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does this accurately explore the parameter space though?\n",
    "# and what is the uncertainty on our data?\n",
    "# what if we had x error bars?\n",
    "\n",
    "# we'll explore a method utilizing the common \"monte carlo\" approach\n",
    "def mcFit(x, y, y_err):\n",
    "    slopes = list()\n",
    "    y_ints = list()\n",
    "    iters = 500 \n",
    "    for i in range(iters):\n",
    "        # remember random normal\n",
    "        weights = np.random.randn(len(y))\n",
    "#         weights2 = np.random.randn(len(data['R_GC']))\n",
    "\n",
    "        y_adj = y + y_err*weights\n",
    "        x_adj = x  # + data['x_err']*weights2\n",
    "\n",
    "        params, other = opt.curve_fit(linear, x_adj, y_adj)\n",
    "        slopes.append(params[0])\n",
    "        y_ints.append(params[1])\n",
    "    \n",
    "    return slopes, y_ints\n",
    "\n",
    "slope, intercept = mcFit(data['R_GC'], data['FE_H'], data['FE_H_ERR'])\n",
    "\n",
    "print('mean slope: {:6.4f}, mean intercept: {:6.4f}'.format(np.mean(slope), np.mean(intercept)))\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'], data['FE_H'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['R_GC'], data['FE_H'],yerr=data['FE_H_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt), label='raw')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt_clean), label='better')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt_err), label='errors')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], np.mean(slope), np.mean(intercept)), label='MC')\n",
    "\n",
    "ax.set_xlabel('R$_{GC}$', fontsize=18)\n",
    "ax.set_ylabel('[Fe/H]', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh right, that outlier.\n",
    "\n",
    "mask = data['FE_H'] < 0.3\n",
    "\n",
    "r_gc = data['R_GC'][mask]\n",
    "fe_h = data['FE_H'][mask]\n",
    "fe_h_err = data['FE_H_ERR'][mask]\n",
    "\n",
    "print(\"data size: \", len(r_gc))\n",
    "\n",
    "slope, intercept = mcFit(r_gc, fe_h, fe_h_err)\n",
    "\n",
    "print('slope: {:+7.4f} $\\pm$ {:6.4f}, mean intercept: {:6.4f}'.format(np.mean(slope), np.std(slope),\n",
    "                                                                          np.mean(intercept)))\n",
    "print(\"\\n\")\n",
    "# lets discuss monte carlo methods a little\n",
    "for i in range(20):\n",
    "    slope, intercept = mcFit(r_gc, fe_h, fe_h_err)\n",
    "    print(\"slope: {:+7.4f} $\\pm$ {:6.4f}, mean intercept: {:6.4f}\".format(np.mean(slope), np.std(slope), \n",
    "                                                                          np.mean(intercept)))\n",
    "\n",
    "# so what we see that our answer changes slightly, but its within the standard deviation\n",
    "# of each set of measurements. So we can quote a reliable answer, with a reliable uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'], data['FE_H'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['R_GC'], data['FE_H'],yerr=data['FE_H_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "slope, intercept = mcFit(r_gc, fe_h, fe_h_err)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt), label='raw')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt_clean), label='better')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt_err), label='errors')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], np.mean(slope), np.mean(intercept)), label='MC')\n",
    "\n",
    "ax.set_xlabel('R$_{GC}$', fontsize=18)\n",
    "ax.set_ylabel('[Fe/H]', fontsize=18)\n",
    "\n",
    "ax.text(10, 0.4, \"Slope: {:+5.3f} $\\pm$ {:5.3f}\".format(np.mean(slope), np.std(slope)), fontsize=18)\n",
    "ax.text(10, 0.35, \"Slope: {:5.3f} $\\pm$ {:5.3f}\".format(np.mean(intercept), np.std(intercept)), fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# here's an example of the use of lampda functions: simplifying or modifying another function\n",
    "params, other = opt.curve_fit(lambda x, b: linear(x, np.mean(slope), b), r_gc, fe_h)\n",
    "\n",
    "fixed_int = params[0]\n",
    "\n",
    "print(fixed_int)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['R_GC'], data['FE_H'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['R_GC'], data['FE_H'],yerr=data['FE_H_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt), label='raw')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt_clean), label='better')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], *popt_err), label='errors')\n",
    "ax.plot(data['R_GC'], linear(data['R_GC'], np.mean(slope), fixed_int), label='MC')\n",
    "\n",
    "ax.set_xlabel('R$_{GC}$', fontsize=18)\n",
    "ax.set_ylabel('[Fe/H]', fontsize=18)\n",
    "\n",
    "ax.text(10, 0.4, \"Slope: {:+5.3f} $\\pm$ {:5.3f}\".format(np.mean(slope), np.std(slope)), fontsize=18)\n",
    "# ax.text(10, 0.35, \"Slope: {:5.3f} $\\pm$ {:5.3f}\".format(np.mean(intercept), np.std(intercept)), fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise: we can't know the x data perfectly\n",
    "# suppose the data have uniform x errors of 0.5\n",
    "# use the MC technique to estimate the slope *AND* the uncertainty taking into account these errors\n",
    "# repeat this exercise, but supposing the errors increase as we move away from R_GC = 7 (as they do)\n",
    "# to be precise: suppose R_GC_ERR = |r_gc-7|/10, e.g. it increases by .1 per kpc (the units of R_GC)\n",
    "\n",
    "# plot both slopes. how does the fit change? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
